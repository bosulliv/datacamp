---
title: "Text mining"
output: html_notebook
---

```{r}
# download datasets
amzn <- read.csv("https://assets.datacamp.com/production/course_935/datasets/500_amzn.csv",
                 stringsAsFactors = FALSE)
goog <- read.csv("https://assets.datacamp.com/production/course_935/datasets/500_goog.csv",
                 stringsAsFactors = FALSE)
```

```{r}
# I added logic to deal with NA's - I replace with a word I know will be removed = Google or Amazon

# Print the structure of amzn
str(amzn)

# Create amzn_pros
amzn_pros <- amzn$pros
i <- which(is.na(amzn_pros))
amzn_pros[i] <- 'Amazon'

# Create amzn_cons
amzn_cons <- amzn$cons
i <- which(is.na(amzn_cons))
amzn_cons[i] <- 'Amazon'

# Print the structure of goog
str(goog)

# Create goog_pros
goog_pros <- goog$pros
i <- which(is.na(goog_pros))
goog_pros[i] <- 'Google'

# Create goog_cons
goog_cons <- goog$cons
i <- which(is.na(goog_cons))
goog_cons[i] <- 'Google'
```


```{r}
# Helper functions

# qdap cleaning function
qdap_clean <- function(x) {
x <- replace_abbreviation(x)
x <- replace_contraction(x)
x <- replace_number(x)
x <- replace_ordinal(x)
x <- replace_symbol(x)
x <- tolower(x)
return(x)
}

# tm cleaning function
tm_clean <- function(corpus) {
tm_clean <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords,
 c(stopwords("en"), "Google", "Amazon", "company"))
return(corpus)
}
```


```{r}
# Alter amzn_pros
amzn_pros <- qdap_clean(amzn_pros)

# Alter amzn_cons
amzn_cons <- qdap_clean(amzn_cons)

# Create az_p_corp 
az_p_corp <- VCorpus(VectorSource(amzn_pros))

# Create az_c_corp
az_c_corp <- VCorpus(VectorSource(amzn_cons))

# Create amzn_pros_corp
amzn_pros_corp <- tm_clean(az_p_corp)

# Create amzn_cons_corp
amzn_cons_corp <-tm_clean(az_c_corp)
```


```{r}
# Apply qdap_clean to goog_pros
goog_pros <- qdap_clean(goog_pros)

# Apply qdap_clean to goog_cons
goog_cons <- qdap_clean(goog_cons)

# Create goog_p_corp
goog_p_corp <- VCorpus(VectorSource(goog_pros))

# Create goog_c_corp
goog_c_corp <- VCorpus(VectorSource(goog_cons))

# Create goog_pros_corp
goog_pros_corp <- tm_clean(goog_p_corp)

# Create goog_cons_corp
goog_cons_corp <- tm_clean(goog_c_corp)
```


```{r}
tokenizer <- function(x) 
  NGramTokenizer(x, Weka_control(min = 2, max = 2))
```

```{r}
# Create amzn_p_tdm
amzn_p_tdm <- TermDocumentMatrix(amzn_pros_corp,
                                 control = list(tokenize = tokenizer))

# Create amzn_p_tdm_m
amzn_p_tdm_m <- as.matrix(amzn_p_tdm)

# Create amzn_p_freq
amzn_p_freq <- rowSums(amzn_p_tdm_m)

# Plot a wordcloud using amzn_p_freq values
wordcloud(names(amzn_p_freq), amzn_p_freq, max.words = 25, color = "blue")
```



```{r}
# Create amzn_c_tdm
amzn_c_tdm <- TermDocumentMatrix(amzn_cons_corp,
                                 control = list(tokenize = tokenizer))

# Create amzn_c_tdm_m
amzn_c_tdm_m <- as.matrix(amzn_c_tdm)

# Create amzn_c_freq
amzn_c_freq <- rowSums(amzn_c_tdm_m)

# Plot a wordcloud of negative Amazon bigrams
wordcloud(names(amzn_c_freq), amzn_c_freq, max.words = 25, color = "red")
```

```{r}
# Create amzn_c_tdm
amzn_c_tdm <- TermDocumentMatrix(amzn_cons_corp,
                                 control = list(tokenize = tokenizer))

# Print amzn_c_tdm to the console
amzn_c_tdm

# Create amzn_c_tdm2 by removing sparse terms 
amzn_c_tdm2 <- removeSparseTerms(amzn_c_tdm, sparse = 0.993)

# Create hc as a cluster of distance values
hc <- hclust(dist(amzn_c_tdm2, method = "euclidean"),
             method = "complete")

# Produce a plot of hc
plot(hc)
```

```{r}
# Create amzn_p_tdm
amzn_p_tdm <- TermDocumentMatrix(amzn_pros_corp,
                                 control = list(tokenize = tokenizer))

# Create amzn_p_m
amzn_p_m <- as.matrix(amzn_p_tdm)

# Create amzn_p_freq
amzn_p_freq <- rowSums(amzn_p_m)

# Create term_frequency
term_frequency <- sort(amzn_p_freq, decreasing = TRUE)

# Print the 5 most common terms
term_frequency[1:5]

# Find associations with fast paced
findAssocs(amzn_p_tdm, "fast paced", 0.2)
```


```{r}
# Prep data
# Apply qdap_clean to goog_pros
goog_pros <- qdap_clean(goog_pros)

# Apply qdap_clean to goog_cons
goog_cons <- qdap_clean(goog_cons)

all_goog_rev <- c(paste(goog_pros, collapse =  " "),
                  paste(goog_cons, collapse = " "))

all_goog_corpus <- VCorpus(VectorSource(all_goog_rev))
```

```{r}
# Create all_goog_corp
all_goog_corp <- tm_clean(all_goog_corpus)

# Create all_tdm
all_tdm <- TermDocumentMatrix(all_goog_corp)

# Name the columns of all_tdm
colnames(all_tdm) <- c("Goog_Pros", "Goog_Cons")

# Create all_m
all_m <- as.matrix(all_tdm)

# Build a comparison cloud
comparison.cloud(all_m, colors = c("#F44336", "#2196f3"), max.words = 100)
```

```{r}
# Prep data
# Apply qdap_clean to goog_cons
amzn_pros <- qdap_clean(amzn_pros)

# Apply qdap_clean to goog_pros
goog_pros <- qdap_clean(goog_pros)

all_pros_rev <- c(paste(amzn_pros, collapse =  " "),
                  paste(goog_pros, collapse = " "))

all_pros_corpus <- VCorpus(VectorSource(all_pros_rev))

# Clean
all_pros_corp <- tm_clean(all_pros_corpus)

# Create all_tdm
all_tdm <- TermDocumentMatrix(all_pros_corp,
                              control = list(tokenize = tokenizer))

# Name the columns of all_tdm
colnames(all_tdm) <- c("Amazon Pro", "Google Pro")

all_tdm_m <- as.matrix(all_tdm)
```

```{r}
# Create common_words
common_words <- subset(all_tdm_m, all_tdm_m[, 1] > 0 & all_tdm_m[, 2] > 0)

# Create difference
difference <- abs(common_words[, 1] - common_words[, 2])

# Add difference to common_words
common_words <- cbind(common_words, difference)

# Order the data frame from most differences to least
common_words <- common_words[order(common_words[, 3], decreasing = TRUE), ]

# Create top15_df
top15_df <- data.frame(x = common_words[1:15, 1], 
                       y = common_words[1:15, 2], 
                       labels = rownames(common_words[1:15, ]))

# Create the pyramid plot
pyramid.plot(top15_df$x, top15_df$y, 
             top15_df$labels, gap = 12, 
             top.labels = c("Amzn", "Pro Words", "Google"), 
             main = "Words in Common", unit = NULL)
```

```{r}
# Prep data
# Apply qdap_clean to goog_cons
amzn_cons <- qdap_clean(amzn_cons)

# Apply qdap_clean to goog_pros
goog_cons <- qdap_clean(goog_cons)

all_cons_rev <- c(paste(amzn_cons, collapse =  " "),
                  paste(goog_cons, collapse = " "))

all_cons_corpus <- VCorpus(VectorSource(all_cons_rev))

# Clean
all_cons_corp <- tm_clean(all_cons_corpus)

# Create all_tdm
all_tdm <- TermDocumentMatrix(all_cons_corp,
                              control = list(tokenize = tokenizer))

# Name the columns of all_tdm
colnames(all_tdm) <- c("Amazon Cons", "Google Cons")

all_tdm_m <- as.matrix(all_tdm)
```

```{r}
# Create common_words
common_words <- subset(all_tdm_m, all_tdm_m[, 1] > 0 & all_tdm_m[, 2] > 0)

# Create difference
difference <- abs(common_words[, 1] - common_words[, 2])

# Add difference to common_words
common_words <- cbind(common_words, difference)

# Order the data frame from most differences to least
common_words <- common_words[order(common_words[, 3], decreasing = TRUE), ]

# Create top15_df
top15_df <- data.frame(x = common_words[1:15, 1], 
                       y = common_words[1:15, 2], 
                       labels = rownames(common_words[1:15, ]))

# Create the pyramid plot
pyramid.plot(top15_df$x, top15_df$y, 
             top15_df$labels, gap = 12, 
             top.labels = c("Amzn", "Cons Words", "Google"), 
             main = "Words in Common", unit = NULL)
```

doh - I've done those not on digrams! fix it! DONE!










